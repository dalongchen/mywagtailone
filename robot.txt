(5,60,1) 5维60行*1列的数组

即 fit_transform 是 fit 和 transform 的组合，整个过程既包括了训练又包含了转换
fit_transform 对数据先拟合 fit，找到数据的整体指标，如均值、方差、最大值最小值等，然后对数据集进行转换transform，
从而实现数据的标准化、归一化操作.不能对训练集和测试集分开使用 fit_transform，虽然这样对测试集也能正常转换（归一化或标准化）
，但是两个结果不是在同一个标准下的，具有明显差异。

plt.rcParams['font.sans-serif'] = ['SimHei']  # 显示中文标签

[i - 60:i, 0]= 0到59共60个。[i, 0]=》 60=i=第61个数 ， 0表示第一列

RNN是什么，递归神经网络(recursive neural network)或者循环神经网络(Recurrent Neural Network)的缩写都是RNN,循环神经网络首先被提出，
而递归神经网络是循环神经网络的推广，不过现在我们一般不加区分，叫什么都可以，两者一般通用

SimpleRNN层不擅长于处理较长的序列，而LSTM则相对于SimpleRNN更适合处理较长的序列。

在keras中，若想进行RNN的堆叠，则return_sequence参数必须设为True，这样返回的才是一系列数据，如果后面不想接RNN，
而是想对某句由多词组合的句子进行进一步分析，则return_sequence可为False。默认为false

LSTM长短期记忆网络Long Short-Term Memory

Dense即全连接层 第一层需要指定输入形状，以对接输入数据的形状，比如以(16,)对接(*,16)形状的数据

“HDF”是“Hierarchical Data Format”的缩写。每个HDF5文件中的对象都有一个名字（name），用/分隔符分隔

卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络
（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一.

与其它神经网络算法类似，由于使用梯度下降算法进行学习，卷积神经网络的输入特征需要进行标准化处理。具体地，
在将学习数据输入卷积神经网络前，需在通道或时间/频率维对输入数据进行归一化，若输入数据为像素，
也可将分布于 的原始像素值归一化至 区间  。输入特征的标准化有利于提升卷积神经网络的学习效率和表现

权值共享意味着每一个过滤器在遍历整个图像的时候，过滤器的参数(即过滤器的参数的值)是固定不变的，比如我有3个特征过滤器，
每个过滤器都会扫描整个图像，在扫描的过程中，过滤器的参数值是固定不变的，即整个图像的所有元素都“共享”了相同的权值

卷积核就是图像处理时，给定输入图像，输入图像中一个小区域中像素加权平均后成为输出图像中的每个对应像素，
其中权值由一个函数定义，这个函数称为卷积核.卷积核卷积后得到的会是原图的某些特征（如边缘信息），所以在 CNN 中，
卷积核卷积得到的 Layer 称作特征图。

梯度呈指数级衰减，导数在每一层至少会被压缩为原来的1/4，当z值绝对值特别大时，导数趋于0，正是因为这两个原因，
从输出层不断向输入层反向传播训练时，导数很容易逐渐变为0，使得权重和偏差参数无法被更新，导致神经网络无法被优化，
训练永远不会收敛到良好的解决方案。 这被称为梯度消失问题

同梯度消失类似，当神经网络很深时，梯度呈指数级增长，最后到输入时，梯度将会非常大，我们会得到一个非常大的权重更新，
这就是梯度爆炸的问题，在循环神经网络中最为常见.

隐马尔可夫模型（Hidden Markov Model，HMM

 基于参数的模型或基于距离的模型，都是要进行特征的归一化。
基于树的方法是不需要进行特征的归一化，例如随机森林，bagging 和 boosting等。

夏普比率的计算非常简单，用基金净值增长率的平均值减无风险利率再除以基金净值增长率的标准差就可以得到基金的夏普比率。
它反映了单位风险基金净值增长率超过无风险收益率的程度。如果夏普比率为正值，
说明在衡量期内基金的平均净值增长率超过了无风险利率，在以同期银行存款利率作为无风险利率的情况下，说明投资基金比银行存款要好。
夏普比率越大，说明基金的单位风险所获得的风险回报越高

 离散形式的傅立叶变换可以利用数字计算机快速地算出（其算法称为快速傅里叶变换算法（FFT))

 预测的过程其实只是基于一个简单的公式,业界称之为逻辑回归：z = dot(w,x) + b  公式中的dot()函数表示将w和x进行向量相乘。
 x代表着输入特征向量，假设只有3个特征，那么x就可以用（x1，x2，x3）来表示。w表示权重，它对应于每个输入特征，
 代表了每个特征的重要程度。b表示阈值[yù zhí]，用来影响预测结果.在逻辑回归公式中，w其实应该写成w.T，
 因为实际运算中我们需要w的转置矩阵

 在实际的神经网络中，我们不能直接用逻辑回归。必须要在逻辑回归外面再套上一个函数。这个函数我们就称它为激活函数。
 激活函数非常非常重要，如果没有它，那么神经网络的智商永远高不起来。而且激活函数又分好多种

 自回归综合移动平均值（ARIMA）模型和扩展.autoregressive integration moving average.ARIMA模型是在ARMA模型的基础上多了差分的操作。
 autoregressive models（自回归模型）: AR( p )
moving average models（移动平均模型）: MA(q)
mixed autoregressive moving average models（混合自回归移动平均模型）: ARMA(p, q)
integration models（整合模型）: ARIMA(p, d, q)
d 阶差分,q几天的平均。
p是模型的自回归部分。 它允许我们将过去价值观的影响纳入我们的模型。 直观地说，这将是类似的，表示如果过去3天已经变暖，明天可能会变暖。
d是模型的集成部分。 这包括模型中包含差异量（即从当前值减去的过去时间点的数量）以适用于时间序列的术语。 直观地说，这将类似于说如果过去三天的温度差异非常小，明天可能会有相同的温度。
q是模型的移动平均部分。 这允许我们将模型的误差设置为过去以前时间点观察到的误差值的线性组合

简单移动平均的公式就是Ft=(At-1+At-2+At-3+…+At-n)/n，其中At-1到At-n、n都是已知的，求Ft即可。而此方法是预测At的，
理论默认是At=Ft的。  加权移动平均就是再简单移动平均上面增加权重，公式变更为Ft=w1At-1+w2At-2+w3At-3+…+wnAt-n，
其中w1...wn是权重，并且w1+...+wn=1。如果w1=w2=...=wn=1/n，那么加权移动平均就变成了简单移动平均。
所以可以理解为简单移动平均就是加权移动平均的特殊情况

等差数列：a1 a2 a3……an……，其中an+1= an + d（ n = 1,2,…n ）d为常数，称为公差， 即 d = an+1 -an , 这就是一个差分,
通常用D(an) = an+1- an来表示，于是有D(an)= d , 这是一个最简单形式的差分方程。
定义. 设变量y依赖于自变量t ,当t变到t + 1时,因变量y = y(t)的改变量Dy(t)= y(t+1) - y(t)称为函数y(t)在点t处步长为1的(一阶)差分，
记作Dy1= yt+1- yt，简称为函数y(t)的(一阶)差分,并称D为差分算子。

对于一组随机变量或者统计数据，其期望值（平均数）用E(X)表示，即随机变量或统计数据的均值， 然后对各个数据与均值的差的 平方和
标准差是方差的平方根.既然有了方差来描述变量与均值的偏离程度，那又搞出来个标准差干什么呢？
原因是方差与我们要处理的数据的量纲是不一致的，虽然能很好的描述数据与均值的偏离程度，但是处理结果是不符合我们的直观思维的。
MSE是真实值与预测值的差值的平方然后求和平均。当预测值与真实值完全相同时为0，误差越大，该值越大。
RMSE （均方根误差）（Root Mean Square Error）